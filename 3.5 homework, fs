1. Так. sparse. Это разрежённые файлы.
Нули не пишутся на диск, а вместо этого инфа и том, насколько смещены эти самые дыры от начала файла и размер дыр, хранится в метаданных ФС.
Польза - размер файлов меньше. Не нужно физически хаписывать нулевые байты, а вот если нужна запись в файл середину файла, то будет фрагментация. Поскольку файл плотненький, писать некуда.
И передавать меньше инфы можно, видимо, если её так организовать, в виде списков дыр. Вот потому торренты, о которых в статье сказано, это и используют.

2. Могут ли файлы, являющиеся жесткой ссылкой на один объект, иметь разные права доступа и владельца?
Нет. Потому что это лишь запись о файле в таблице и inode имеют тот же, что и сам файл. Вот soft-link могут, потому что они - сами по себе файлы со своими inode и им можно атрибуты установить.

3. Готово. Вот:
vagrant@vagrant:~$ lsblk
NAME                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                    8:0    0   64G  0 disk
├─sda1                 8:1    0  512M  0 part /boot/efi
├─sda2                 8:2    0    1K  0 part
└─sda5                 8:5    0 63.5G  0 part
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm  /
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm  [SWAP]
sdb                    8:16   0  2.5G  0 disk
sdc                    8:32   0  2.5G  0 disk

Это она.

4. Пустим fdisk и дадим ему sbd
sudo fdisk /dev/sdb
Далее интерактивно:
o - создадим пустую таблицу разделов MBR
n - создать раздел
p - основной
1 - первый из 4-х возможных
с 2048 сектора
+2G (чтобы не считать, по какой сектор, пусть само)
n - ещё один раздел
p - основной
2 - второй
первый и последний секторы по умолчанию. Этот будет оставшееся место.
Посмотрим, что получилось:
Command (m for help): p
Disk /dev/sdb: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x41cbb2e5

Device     Boot   Start     End Sectors  Size Id Type
/dev/sdb1          2048 4196351 4194304    2G 83 Linux
/dev/sdb2       4196352 5242879 1046528  511M 83 Linux

Норм. Теперь
w - применить изменения
q - выйти вон

5. Находим в man sfdisk вот что:
       -d, --dump device
              Dump the partitions of a device in a format that is usable as input to sfdisk.  See the section BACKING UP THE PARTITION TABLE.
Дамп одного устройства передаём sfdisk и даём ему другое устройство.
То есть, делаем так:
sudo sfdisk -d /dev/sdb | sfdisk /dev/sdc
И имеем:
root@vagrant:~# lsblk
NAME                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                    8:0    0   64G  0 disk
├─sda1                 8:1    0  512M  0 part /boot/efi
├─sda2                 8:2    0    1K  0 part
└─sda5                 8:5    0 63.5G  0 part
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm  /
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm  [SWAP]
sdb                    8:16   0  2.5G  0 disk
├─sdb1                 8:17   0    2G  0 part
└─sdb2                 8:18   0  511M  0 part
sdc                    8:32   0  2.5G  0 disk
├─sdc1                 8:33   0    2G  0 part
└─sdc2                 8:34   0  511M  0 part

6. 7.
В общем, вот:
root@vagrant:~# mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
Continue creating array? y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
root@vagrant:~# mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb2 /dev/sdc2
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
root@vagrant:~# lsblk
NAME                 MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                    8:0    0   64G  0 disk
├─sda1                 8:1    0  512M  0 part  /boot/efi
├─sda2                 8:2    0    1K  0 part
└─sda5                 8:5    0 63.5G  0 part
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm   /
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm   [SWAP]
sdb                    8:16   0  2.5G  0 disk
├─sdb1                 8:17   0    2G  0 part
│ └─md1                9:1    0    2G  0 raid1
└─sdb2                 8:18   0  511M  0 part
  └─md0                9:0    0 1018M  0 raid0
sdc                    8:32   0  2.5G  0 disk
├─sdc1                 8:33   0    2G  0 part
│ └─md1                9:1    0    2G  0 raid1
└─sdc2                 8:34   0  511M  0 part
  └─md0                9:0    0 1018M  0 raid0

8. Инициализируем разделы для lvm
root@vagrant:~# pvcreate /dev/md0 /dev/md1
  Physical volume "/dev/md0" successfully created.
  Physical volume "/dev/md1" successfully created.

9. Делаем из них группу разделов (volume group, vg):
root@vagrant:~# vgcreate vl_grp1 /dev/md0 /dev/md1
  Volume group "vl_grp1" successfully created

root@vagrant:~# vgdisplay
  --- Volume group ---
  VG Name               vgvagrant
  System ID
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  3
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                2
  Open LV               2
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               <63.50 GiB
  PE Size               4.00 MiB
  Total PE              16255
  Alloc PE / Size       16255 / <63.50 GiB
  Free  PE / Size       0 / 0
  VG UUID               PaBfZ0-3I0c-iIdl-uXKt-JL4K-f4tT-kzfcyE

  --- Volume group ---
  VG Name               vl_grp1
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               <2.99 GiB
  PE Size               4.00 MiB
  Total PE              765
  Alloc PE / Size       0 / 0
  Free  PE / Size       765 / <2.99 GiB
  VG UUID               B3tiJx-Vqf9-7poB-xSNb-P48y-AJ8e-SLFGGf

10. Делаем теперь логический раздел на /dev/md0. И назовём его (-n) logic_vol1
root@vagrant:~# lvcreate -L 100M -n logic_vol1 vl_grp1 /dev/md0
  Logical volume "logic_vol1" created.
lvdisplay показало, что получилось:
  --- Logical volume ---
  LV Path                /dev/vl_grp1/logic_vol1
  LV Name                logic_vol1
  VG Name                vl_grp1
  LV UUID                KKyRJX-ADnf-Oe9i-eo2g-00pz-0uhx-yEIPUx
  LV Write Access        read/write
  LV Creation host, time vagrant, 2021-11-26 19:13:47 +0000
  LV Status              available
  # open                 0
  LV Size                100.00 MiB
  Current LE             25
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     4096
  Block device           253:2

11.
Делаем ФС:
root@vagrant:~# mkfs.ext4 -t ext4 -L FS_on_lv1 /dev/vl_grp1/logic_vol1
mke2fs 1.45.5 (07-Jan-2020)
Creating filesystem with 25600 4k blocks and 25600 inodes

Allocating group tables: done
Writing inode tables: done
Creating journal (1024 blocks): done
Writing superblocks and filesystem accounting information: done

12.
Делаем директорию, куда будем монтировать нашу ФС:
root@vagrant:~# mkdir /tmp/new

и монтируем:
root@vagrant:~# mount /dev/vl_grp1/logic_vol1 /tmp/new

13.
root@vagrant:~# wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz
--2021-11-26 19:25:19--  https://mirror.yandex.ru/ubuntu/ls-lR.gz
Resolving mirror.yandex.ru (mirror.yandex.ru)... 213.180.204.183, 2a02:6b8::183
Connecting to mirror.yandex.ru (mirror.yandex.ru)|213.180.204.183|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 22607379 (22M) [application/octet-stream]
Saving to: ‘/tmp/new/test.gz’

/tmp/new/test.gz                100%[====================================================>]  21.56M  11.0MB/s    in 2.0s

2021-11-26 19:25:21 (11.0 MB/s) - ‘/tmp/new/test.gz’ saved [22607379/22607379]

14.
root@vagrant:~# lsblk
NAME                     MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                        8:0    0   64G  0 disk
├─sda1                     8:1    0  512M  0 part  /boot/efi
├─sda2                     8:2    0    1K  0 part
└─sda5                     8:5    0 63.5G  0 part
  ├─vgvagrant-root       253:0    0 62.6G  0 lvm   /
  └─vgvagrant-swap_1     253:1    0  980M  0 lvm   [SWAP]
sdb                        8:16   0  2.5G  0 disk
├─sdb1                     8:17   0    2G  0 part
│ └─md1                    9:1    0    2G  0 raid1
└─sdb2                     8:18   0  511M  0 part
  └─md0                    9:0    0 1018M  0 raid0
    └─vl_grp1-logic_vol1 253:2    0  100M  0 lvm   /tmp/new
sdc                        8:32   0  2.5G  0 disk
├─sdc1                     8:33   0    2G  0 part
│ └─md1                    9:1    0    2G  0 raid1
└─sdc2                     8:34   0  511M  0 part
  └─md0                    9:0    0 1018M  0 raid0
    └─vl_grp1-logic_vol1 253:2    0  100M  0 lvm   /tmp/new

15. root@vagrant:~# gzip -t /tmp/new/test.gz
root@vagrant:~# echo $?
0

16.
root@vagrant:~# pvmove /dev/md0
  /dev/md0: Moved: 88.00%
Команда освобожает указанный pv на оставшееся место в его группе томов. В нашем случае - это md1, ибо больше ничего нет.
Вот:
root@vagrant:~# lsblk
NAME                     MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                        8:0    0   64G  0 disk
├─sda1                     8:1    0  512M  0 part  /boot/efi
├─sda2                     8:2    0    1K  0 part
└─sda5                     8:5    0 63.5G  0 part
  ├─vgvagrant-root       253:0    0 62.6G  0 lvm   /
  └─vgvagrant-swap_1     253:1    0  980M  0 lvm   [SWAP]
sdb                        8:16   0  2.5G  0 disk
├─sdb1                     8:17   0    2G  0 part
│ └─md1                    9:1    0    2G  0 raid1
│   └─vl_grp1-logic_vol1 253:2    0  100M  0 lvm   /tmp/new
└─sdb2                     8:18   0  511M  0 part
  └─md0                    9:0    0 1018M  0 raid0
sdc                        8:32   0  2.5G  0 disk
├─sdc1                     8:33   0    2G  0 part
│ └─md1                    9:1    0    2G  0 raid1
│   └─vl_grp1-logic_vol1 253:2    0  100M  0 lvm   /tmp/new
└─sdc2                     8:34   0  511M  0 part
  └─md0                    9:0    0 1018M  0 raid0

17. в man английским по чёрному писано, что
       --fail, -f
              This  allows  the  hot-plug  system to remove devices that have fully disappeared from the kernel.  It will
              first fail and then remove the device from any array it belongs to.  The device name given should be a ker‐
              nel device name such as "sda", not a name in /dev.

root@vagrant:~# mdadm /dev/md1 --fail /dev/sdc1
mdadm: set /dev/sdc1 faulty in /dev/md1

В массиве md1 сдохло sdc1.

18. Проверим, так ли это:
root@vagrant:~# dmesg | grep md1
[ 1638.279051] md/raid1:md1: not clean -- starting background reconstruction
[ 1638.279053] md/raid1:md1: active with 2 out of 2 mirrors
[ 1638.279099] md1: detected capacity change from 0 to 2144337920
[ 1638.289588] md: resync of RAID array md1
[ 1648.847350] md: md1: resync done.
[ 4927.238389] md/raid1:md1: Disk failure on sdc1, disabling device.
               md/raid1:md1: Operation continuing on 1 devices.

Вот, написано, что так. sdc отключен, md1 работает на одном устройстве.

19.
root@vagrant:~# gzip -t /tmp/new/test.gz
root@vagrant:~# echo $?
0
Порядок.

20. Гасим хост
[cats@host-63 vagrant_config]$ vagrant destroy
    default: Are you sure you want to destroy the 'default' VM? [y/N] y
==> default: Forcing shutdown of VM...
==> default: Destroying VM and associated drives...
[cats@host-63 vagrant_config]$
и идём спать.
